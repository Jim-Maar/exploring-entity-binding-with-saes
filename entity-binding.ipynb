{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Entity Binding Circuit with SAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "# import circuitsvis as cv\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import circuitsvis as cv\n",
    "import transformer_lens\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top of your notebook if not already present\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    # Release all references to tensors\n",
    "    # model.cpu()  # Move model to CPU\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "    gc.collect()  # Run garbage collection\n",
    "    \n",
    "    # Print memory stats to verify\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Call this function whenever you need to clear memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# After clearing, you can move the model back to GPU if needed\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating The Paper results with Gemma 2B\n",
    "- Load Model (Done)\n",
    "- Test Model on Capitals Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f97139c75f499a83289c93b3074289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('google/gemma-2-2b-it', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = model.to(device)\\ngemmascope_sae_release = \"gemma-scope-2b-pt-res-canonical\"\\ngemmascope_sae_id = \"layer_20/width_16k/canonical\"\\ngemma_2_2b_sae = SAE.from_pretrained(gemmascope_sae_release, gemmascope_sae_id, device=str(device))[0]\\nlatent_idx = 12082\\n\\ndisplay_dashboard(sae_release=gemmascope_sae_release, sae_id=gemmascope_sae_id, latent_idx=latent_idx)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = model.to(device)\n",
    "gemmascope_sae_release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "gemmascope_sae_id = \"layer_20/width_16k/canonical\"\n",
    "gemma_2_2b_sae = SAE.from_pretrained(gemmascope_sae_release, gemmascope_sae_id, device=str(device))[0]\n",
    "latent_idx = 12082\n",
    "\n",
    "display_dashboard(sae_release=gemmascope_sae_release, sae_id=gemmascope_sae_id, latent_idx=latent_idx)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 37, 49, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_list = ['Thailand', 'Japan', 'Brazil', 'Morocco', 'Sweden', 'Kenya', 'Argentina', 'Australia', 'Egypt', 'Canada', 'Vietnam', 'Portugal', 'India', 'Norway', 'Mexico', 'Malaysia', 'Greece', 'Finland', 'Indonesia', 'Turkey', 'Chile', 'Ireland', 'Bangladesh', 'Denmark', 'Peru', 'Iceland', 'Colombia', 'Singapore', 'Austria', 'Nigeria', 'Croatia', 'Taiwan', 'Switzerland', 'Ghana', 'Cambodia', 'Poland', 'Nepal', 'Uruguay', 'Tanzania', 'Belgium', 'Jordan', 'Hungary', 'Bhutan', 'Maldives', 'Venezuela', 'Laos', 'Romania', 'Somalia', 'Mongolia', 'Uzbekistan']\n",
    "name_list = ['Emma', 'James', 'Luna', 'Kai', 'Zara', 'Leo', 'Maya', 'Finn', 'Nova', 'Atlas', 'Rose', 'Sage', 'Jack', 'Ruby', 'Owen', 'Grace', 'Dean', 'Hope', 'Blake', 'Dawn', 'Cole', 'Faith', 'Reed', 'Sky', 'Jade', 'Wolf', 'Rain', 'Quinn', 'Blaze', 'Pearl', 'Felix', 'Iris', 'Seth', 'Dove', 'Drake', 'Joy', 'Axel', 'Fern', 'Stone', 'Wren', 'Grant', 'Hazel', 'Brooks', 'Ash', 'Reid', 'Sage', 'Clark', 'Skye', 'Blair', 'Scout']\n",
    "capital_list = ['Bangkok', 'Tokyo', 'Brasilia', 'Rabat', 'Stockholm', 'Nairobi', 'Buenos Aires', 'Canberra', 'Cairo', 'Ottawa', 'Hanoi', 'Lisbon', 'New Delhi', 'Oslo', 'Mexico City', 'Kuala Lumpur', 'Athens', 'Helsinki', 'Jakarta', 'Ankara', 'Santiago', 'Dublin', 'Dhaka', 'Copenhagen', 'Lima', 'Reykjavik', 'Bogota', 'Singapore', 'Vienna', 'Abuja', 'Zagreb', 'Taipei', 'Bern', 'Accra', 'Phnom Penh', 'Warsaw', 'Kathmandu', 'Montevideo', 'Dodoma', 'Brussels', 'Amman', 'Budapest', 'Thimphu', 'Male', 'Caracas', 'Vientiane', 'Bucharest', 'Mogadishu', 'Ulaanbaatar', 'Tashkent']\n",
    "capital_list = [f\" {capital}\" for capital in capital_list]\n",
    "fruit_list = ['orange', 'zucchini', 'xigua', 'uva', 'papaya', 'elderberry', 'banana', 'fig', 'vanilla', 'mango', 'quince', 'pomegranate', 'yuzu', 'lemon', 'peach', 'saffron', 'date', 'strawberry', 'kiwi', 'grape', 'honeydew', 'quinoa', 'pear', 'rhubarb', 'cherry', 'raspberry', 'apple', 'tangerine', 'pineapple', 'plum', 'tomato', 'watermelon', 'prune', 'raisin']\n",
    "fruit_list = [f\" {fruit}\" for fruit in fruit_list]\n",
    "# country_list = [f\" {country}\" for country in country_list]\n",
    "# name_list = [f\" {name}\" for name in name_list]\n",
    "\n",
    "def get_one_token_lists(lists):\n",
    "    assert len(lists) > 0\n",
    "    assert type(lists[0]) is list\n",
    "    list_len = len(lists[0])\n",
    "    assert all([len(l) == list_len for l in lists])\n",
    "    one_token_indices = []\n",
    "    for i in range(list_len):\n",
    "        is_one_token = True\n",
    "        for j in range(len(lists)):\n",
    "            name = lists[j][i]\n",
    "            toks = model.to_tokens(name, prepend_bos=False)[0]\n",
    "            if len(toks) > 1:\n",
    "                is_one_token = False\n",
    "                break\n",
    "            if name[0] == ' ':\n",
    "                continue\n",
    "            toks = model.to_tokens(f\" {name}\", prepend_bos=False)[0]\n",
    "            if len(toks) > 1:\n",
    "                is_one_token = False\n",
    "                break\n",
    "        if is_one_token:\n",
    "            one_token_indices += [i]\n",
    "    new_lists = [[] for _ in range(len(lists))]\n",
    "    for idx in one_token_indices:\n",
    "        for j in range(len(lists)):\n",
    "            new_lists[j] += [lists[j][idx]]\n",
    "    return new_lists\n",
    "\n",
    "name_list_one_token = get_one_token_lists([name_list])[0]\n",
    "country_list_one_token, capital_list_one_token = get_one_token_lists([country_list, capital_list])\n",
    "fruit_list = get_one_token_lists([fruit_list])[0]\n",
    "len(country_list_one_token), len(capital_list_one_token), len(name_list_one_token), len(fruit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 56]) torch.Size([100]) torch.Size([100])\n",
      "torch.Size([100, 47]) torch.Size([100]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "def get_prompt_capital(qn_subject, entities, attributes):\n",
    "    assert len(entities) == len(attributes)\n",
    "    n = len(entities)\n",
    "    return \"Answer the question based on the context below. Keep the answer short.\\n\" + \\\n",
    "\"Context: \" + \"\\n\".join([f'{entities[i]} lives in the capital city of{attributes[i]}.' for i in range(n)]) + '\\n' + \\\n",
    "f\"Question: Which city does {qn_subject} live in?\\n\" + \\\n",
    "f\"Answer: {qn_subject} lives in the city of\"\n",
    "\n",
    "def get_prompt_fruit(qn_subject, entities, attributes):\n",
    "    assert len(entities) == len(attributes)\n",
    "    n = len(entities)\n",
    "    return \"Answer the question based on the context below. Keep the answer short.\\n\" + \\\n",
    "\"Context: \" + \". \".join([f\"{entities[i]} likes eating the{attributes[i]}\" for i in range(n)]) + \"\\nrespectively.\\n\" + \\\n",
    "f\"Question: What food does {qn_subject} like?\\n\" + \\\n",
    "f\"Answer: {qn_subject} likes the\"\n",
    "\n",
    "def get_prompts(batch_size, entities, attributes, get_prompt_function, answer_list = None, num_pairs = 2):\n",
    "    # sample n pairs of country+capital and n pairs of name+name\n",
    "    prompts = []\n",
    "    correct_answers = []\n",
    "    incorrect_answers = []\n",
    "    \n",
    "    # Sample n countries and n names without replacement\n",
    "    entities_list = []\n",
    "    attributes_list = []\n",
    "    for i in range(batch_size):\n",
    "        ents = random.sample(entities, num_pairs)\n",
    "        attrs = random.sample(attributes, num_pairs)\n",
    "        entities_list.append(ents)\n",
    "        attributes_list.append(attrs)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # For each prompt, we need 2 different countries and 2 different names\n",
    "        ents = entities_list[i]\n",
    "        attrs = attributes_list[i]\n",
    "        qn_subject = random.choice(ents)\n",
    "        correct_entity_idx = ents.index(qn_subject)\n",
    "        attribute = attrs[correct_entity_idx]\n",
    "        incorrect_attributes = attrs.copy()\n",
    "        incorrect_attributes.remove(attribute)\n",
    "        incorrect_attribute = random.choice(incorrect_attributes)\n",
    "        if answer_list is not None:\n",
    "            correct_answer = answer_list[attributes.index(attribute)]\n",
    "            incorrect_answer = answer_list[attributes.index(incorrect_attribute)]\n",
    "        else:\n",
    "            correct_answer = attribute\n",
    "            incorrect_answer = incorrect_attribute\n",
    "\n",
    "        prompt = get_prompt_function(qn_subject, ents, attrs)\n",
    "        prompts.append(prompt)\n",
    "        correct_answers.append(correct_answer)\n",
    "        incorrect_answers.append(incorrect_answer)\n",
    "    return prompts, correct_answers, incorrect_answers\n",
    "\n",
    "n = 100\n",
    "num_pairs = 2\n",
    "prompts_capital, correct_answers_capital, incorrect_answers_capital = get_prompts(n, name_list_one_token, country_list_one_token, get_prompt_capital, capital_list_one_token, num_pairs)\n",
    "prompts_fruit, correct_answers_fruit, incorrect_answers_fruit = get_prompts(n, name_list_one_token, fruit_list, get_prompt_fruit, num_pairs=num_pairs)\n",
    "input_toks_capital = model.to_tokens(prompts_capital)\n",
    "correct_toks_capital = model.to_tokens(correct_answers_capital, prepend_bos=False)[:, 0]\n",
    "incorrect_toks_capital = model.to_tokens(incorrect_answers_capital, prepend_bos=False)[:, 0]\n",
    "input_toks_fruit = model.to_tokens(prompts_fruit)\n",
    "correct_toks_fruit = model.to_tokens(correct_answers_fruit, prepend_bos=False)[:, 0]\n",
    "incorrect_toks_fruit = model.to_tokens(incorrect_answers_fruit, prepend_bos=False)[:, 0]\n",
    "\n",
    "print(input_toks_capital.shape, correct_toks_capital.shape, incorrect_toks_capital.shape)\n",
    "print(input_toks_fruit.shape, correct_toks_fruit.shape, incorrect_toks_fruit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc8f2dd17f04a9f8f1dc6f1493557fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9900, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = model.generate(input_toks_capital, max_new_tokens=1, do_sample=False)\n",
    "model_out = toks[:, -1]\n",
    "correct = (model_out == correct_toks_capital)\n",
    "acc = correct.float().mean()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toks = model.generate(input_toks_capital, max_new_tokens=5, do_sample=False)\n",
    "# model.to_str_tokens(toks[0]), model.to_str_tokens(correct_toks_capital[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.4252, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(input_toks_fruit)[:, -1, :]\n",
    "correct_logits = logits[t.arange(logits.shape[0]), correct_toks_fruit]\n",
    "incorrect_logits = logits[t.arange(logits.shape[0]), incorrect_toks_fruit]\n",
    "logit_diff = correct_logits - incorrect_logits\n",
    "logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), torch.Size([100]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.sum(dim=-1).shape, logits[t.arange(logits.shape[0]), correct_toks_capital].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Leo Vietnam Dean Peru\n",
      "['<bos>', 'Answer', ' the', ' question', ' based', ' on', ' the', ' context', ' below', '.', ' Keep', ' the', ' answer', ' short', '.', '\\n', 'Context', ':', ' Hope', ' likes', ' eating', ' the', ' kiwi', '.', ' Seth', ' likes', ' eating', ' the', ' apple', '\\n', 'respectively', '.', '\\n', 'Question', ':', ' What', ' food', ' does', ' Seth', ' like', '?', '\\n', 'Answer', ':', ' Seth', ' likes', ' the']\n",
      " Hope  kiwi  Seth  apple\n"
     ]
    }
   ],
   "source": [
    "str_tokens_capital = model.to_str_tokens(prompts_capital[1])\n",
    "ENTITY_POSITIONS_CAPITAL = [18 + 10 * i for i in range(num_pairs)]\n",
    "ATTRIBUTE_POSITIONS_CAPITAL = [25 + 10 * i for i in range(num_pairs)]\n",
    "E_0_POS_CAPITAL, A_0_POS_CAPITAL, E_1_POS_CAPITAL, A_1_POS_CAPITAL = ENTITY_POSITIONS_CAPITAL[0], ATTRIBUTE_POSITIONS_CAPITAL[0], ENTITY_POSITIONS_CAPITAL[1], ATTRIBUTE_POSITIONS_CAPITAL[1]\n",
    "print(str_tokens_capital[E_0_POS_CAPITAL], str_tokens_capital[A_0_POS_CAPITAL], str_tokens_capital[E_1_POS_CAPITAL], str_tokens_capital[A_1_POS_CAPITAL])\n",
    "str_tokens_fruit = model.to_str_tokens(prompts_fruit[1])\n",
    "print(str_tokens_fruit)\n",
    "ENTITY_POSITIONS_FRUIT = [18 + 6 * i for i in range(num_pairs)]\n",
    "ATTRIBUTE_POSITIONS_FRUIT = [22 + 6 * i for i in range(num_pairs)]\n",
    "E_0_POS_FRUIT, A_0_POS_FRUIT, E_1_POS_FRUIT, A_1_POS_FRUIT = ENTITY_POSITIONS_FRUIT[0], ATTRIBUTE_POSITIONS_FRUIT[0], ENTITY_POSITIONS_FRUIT[1], ATTRIBUTE_POSITIONS_FRUIT[1]\n",
    "print(str_tokens_fruit[E_0_POS_FRUIT], str_tokens_fruit[A_0_POS_FRUIT], str_tokens_fruit[E_1_POS_FRUIT], str_tokens_fruit[A_1_POS_FRUIT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([26, 2304]), torch.Size([26, 2304]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, cache = model.run_with_cache(\n",
    "    input_toks_capital,\n",
    "    return_type=None,\n",
    "    names_filter=lambda name: \"resid_post\" in name,\n",
    ")\n",
    "\n",
    "e_0_activations = t.stack([cache[utils.get_act_name(\"resid_post\", layer=layer)][:, E_0_POS_CAPITAL, :].mean(dim=0) for layer in range(model.cfg.n_layers)])\n",
    "a_0_activations = t.stack([cache[utils.get_act_name(\"resid_post\", layer=layer)][:, A_0_POS_CAPITAL, :].mean(dim=0) for layer in range(model.cfg.n_layers)])\n",
    "e_1_activations = t.stack([cache[utils.get_act_name(\"resid_post\", layer=layer)][:, E_1_POS_CAPITAL, :].mean(dim=0) for layer in range(model.cfg.n_layers)])\n",
    "a_1_activations = t.stack([cache[utils.get_act_name(\"resid_post\", layer=layer)][:, A_1_POS_CAPITAL, :].mean(dim=0) for layer in range(model.cfg.n_layers)])\n",
    "\n",
    "b_E_diff = e_0_activations - e_1_activations\n",
    "b_A_diff = a_0_activations - a_1_activations\n",
    "\n",
    "def substract_binding_diff_capital_hook(resid, layer, hook: HookPoint):\n",
    "    resid[:, E_1_POS_CAPITAL, :] -= b_E_diff[layer]\n",
    "    resid[:, A_1_POS_CAPITAL, :] -= b_A_diff[layer]\n",
    "    return resid\n",
    "\n",
    "def substract_binding_diff_fruit_hook(resid, layer, hook: HookPoint):\n",
    "    resid[:, E_1_POS_FRUIT, :] -= b_E_diff[layer]\n",
    "    resid[:, A_1_POS_FRUIT, :] -= b_A_diff[layer]\n",
    "    return resid\n",
    "\n",
    "b_E_diff.shape, b_A_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8369, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.run_with_hooks(input_toks_capital, fwd_hooks=[\n",
    "    (utils.get_act_name(\"resid_post\", layer=layer), functools.partial(substract_binding_diff_capital_hook, layer=layer))\n",
    "    for layer in range(model.cfg.n_layers)\n",
    "])[:, -1, :]\n",
    "correct_logits = logits[t.arange(logits.shape[0]), correct_toks_capital]\n",
    "incorrect_logits = logits[t.arange(logits.shape[0]), incorrect_toks_capital]\n",
    "logit_diff = correct_logits - incorrect_logits\n",
    "logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0],\n",
      "        [ 2,  1],\n",
      "        [ 0,  1],\n",
      "        [ 0, -1],\n",
      "        [-2,  2]], dtype=torch.int32)\n",
      "tensor([[-1.6516, -0.2583, -0.7595,  0.0370,  0.6003,  0.1623,  1.2059,  0.2307,\n",
      "          0.4935,  0.1546],\n",
      "        [-0.4971, -0.8542, -0.4952, -0.2030,  0.5563,  0.8653, -1.9967, -0.0750,\n",
      "          0.0040, -1.3557],\n",
      "        [ 2.1722, -0.8123,  0.0181,  0.1670, -1.1579,  0.4281,  0.7602,  0.1197,\n",
      "         -0.6030, -0.7378],\n",
      "        [ 0.4848,  0.6359,  1.9388,  1.3068, -3.0513, -1.1464, -0.3492, -0.5236,\n",
      "         -0.9946,  0.8107],\n",
      "        [-1.5078, -0.5990,  0.3570,  0.5402, -0.1357, -2.0495,  0.8681, -0.3213,\n",
      "         -0.5957, -1.9295]])\n",
      "tensor([[ 0.1546, -1.6516],\n",
      "        [-0.4952, -0.8542],\n",
      "        [ 2.1722, -0.8123],\n",
      "        [ 0.4848,  0.8107],\n",
      "        [-0.5957,  0.3570]])\n"
     ]
    }
   ],
   "source": [
    "A = t.randn(size=(5, 2)).round().to(t.int32)\n",
    "B = t.randn(size=(5, 10))\n",
    "print(A)\n",
    "print(B)\n",
    "print(t.stack([B[t.arange(5), A[:, 0]], B[t.arange(5), A[:, 1]]], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.stack([correct_toks_fruit, incorrect_toks_fruit], dim=-1).shape\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6200000047683716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.7398, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.run_with_hooks(input_toks_fruit, fwd_hooks=[\n",
    "    (utils.get_act_name(\"resid_post\", layer=layer), functools.partial(substract_binding_diff_fruit_hook, layer=layer))\n",
    "    for layer in range(model.cfg.n_layers)\n",
    "])[:, -1, :]\n",
    "valid_tokens = t.stack([correct_toks_fruit, incorrect_toks_fruit], dim=-1)\n",
    "correct_logits = logits[t.arange(logits.shape[0]), valid_tokens[:, 0]]\n",
    "incorrect_logits = logits[t.arange(logits.shape[0]), valid_tokens[:, 1]]\n",
    "max_logits = t.stack([correct_logits, incorrect_logits], dim=-1).argmax(dim=-1)\n",
    "correct = (max_logits == 0)\n",
    "acc = correct.float().mean()\n",
    "print(f\"Accuracy: {acc}\")\n",
    "correct_logits = logits[t.arange(logits.shape[0]), correct_toks_fruit]\n",
    "incorrect_logits = logits[t.arange(logits.shape[0]), incorrect_toks_fruit]\n",
    "logit_diff = correct_logits - incorrect_logits\n",
    "logit_diff.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
